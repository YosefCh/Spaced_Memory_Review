# enter you file path to the file that contains your Api key or you can use an environment variable
with open('C:/Users/Rebecca/OneDrive/Documents/Python AI/LLM (AI) Browser History Analyzer/Api_key.txt', 'r') as f:
    # set a constant variable to the key
    API_KEY = f.readline().strip()


# Class for interacting with the OpenAI models that are ideal for general purpose tasks
class OpenAIClient:
    """
    A client for interacting with the OpenAI API.

    Attributes:
        api_key (str): The API key for authenticating with the OpenAI API.
        model_name (str): The name of the model to use for generating responses.
        max_tokens (int): The maximum number of tokens to generate in the response.
        system_role_content (str): The content for the system role in the conversation.
        temperature (float): The randomness of the model's responses.
        top_p (float): The cumulative probability threshold for top-p sampling.

    Methods:
        get_response(prompt: str) -> str:
            Generates a response from the OpenAI API based on the given prompt.
    """
 
    def __init__(self, api_key=API_KEY, 
                 model_name="gpt-5-chat-latest", 
                 max_tokens=4096, 
                 system_role_content="You are a helpful assistant.", 
                 temperature=.5, 
                 top_p=.8):
        """
        Initializes the OpenAIClient with the given parameters.

        Args:
            api_key (str): The API key for authenticating with the OpenAI API.
            model_name (str): The name of the model to use for generating responses. Defaults to "gpt-4o-mini".
            max_tokens (int): The maximum number of tokens to generate in the response. Defaults to 4096.
            system_role_content (str): The content for the system role in the conversation. Defaults to "You are a helpful assistant.".
            temperature (float): The randomness of the model's responses. We chose a default of 0.3
            top_p (float): The cumulative probability threshold for top-p sampling. We chosse a default of 0.6
        """
        import openai
        openai.api_key = api_key
        self.client = openai
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.system_role_content = system_role_content
        self.temperature = temperature
        self.top_p = top_p

    def get_response(self, prompt):
        """
        Generates a response from the OpenAI API based on the given prompt.

        Args:
            prompt (str): The prompt to send to the OpenAI API.

        Returns:
            str: The response generated by the OpenAI API.
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_role_content},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                top_p=self.top_p
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(e)
            

# Class for interacting with the OpenAI models that are ideal for more complex reasoning tasks
class Reasoning_OpenAIClient:
    """
    A client for interacting with the OpenAI API.

    Attributes:
        api_key (str): The API key for authenticating with the OpenAI API.
        model_name (str): The name of the model to use for generating responses.
        reasoning (str): The level of reasoning capability ("low", "medium", or "high").
        system_role_content (str): The content for the system role in the conversation.

    Notes:
        - The model has a context window of 128K tokens
        - Can generate up to 16.4K tokens in output
        - Default max_completion_tokens is 512 if not specified
        - Token usage includes both visible output tokens and reasoning tokens
    """
 
    def __init__(self, api_key=API_KEY, 
                 model_name="o4-mini", 
                 reasoning = "medium",
                 system_role_content="You are a helpful assistant." 
                 ):
        """
        Initializes the OpenAIClient with the given parameters.

        Args:
            api_key (str): The API key for authenticating with the OpenAI API.
            model_name (str): The name of the model to use for generating responses. Defaults to "gpt-o4-mini".
            reasoning (str): The level of reasoning capability ("low", "medium", or "high"). Defaults to "medium".
                           Lower levels use fewer tokens for reasoning but may be less thorough.
            system_role_content (str): The content for the system role in the conversation. Defaults to "You are a helpful assistant.".

        Note:
            The model will automatically manage token usage for both output and internal reasoning.
            You can control reasoning token usage through the reasoning parameter.
        """
        import openai
        openai.api_key = api_key
        self.client = openai
        self.model_name = model_name
        self.reasoning = reasoning
        self.system_role_content = system_role_content

    def get_response(self, prompt):
        """
        Generates a response from the OpenAI API based on the given prompt.

        Args:
            prompt (str): The prompt to send to the OpenAI API.

        Returns:
            str: The response generated by the OpenAI API.

        Note:
            The response length and reasoning depth are automatically managed based on:
            - The reasoning level set during initialization ("low", "medium", "high")
            - The model's default max_completion_tokens (512)
            - Available context window (128K tokens)
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_role_content},
                    {"role": "user", "content": prompt}
                ]
              

            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(e)
            
            
# if __name__ == '__main__':
    
    
    
    